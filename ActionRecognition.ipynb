{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTGtUIc1neV3",
        "outputId": "36270ee9-6ab4-4c99-a416-e14958496091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdu-VOaUKma4"
      },
      "outputs": [],
      "source": [
        "#Required Imports\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('AGG')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "from keras.layers import (Input, Activation, Conv3D, Dense, Dropout, Flatten, MaxPooling3D, Input, average, BatchNormalization, LeakyReLU)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptgr_XSHXmi",
        "outputId": "bf8b9269-1d19-4455-dc0e-a4a6cdeaeda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 0\n"
          ]
        }
      ],
      "source": [
        "# Check GPU Availability\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Reduce Precision From float32 To float16\n",
        "# from tensorflow.keras import mixed_precision\n",
        "# mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "#Accelerated Linear Algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "# Create An Optimizer With Loss Scaling To Prevent Precision Issues\n",
        "# from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# opt = LossScaleOptimizer(Adam(learning_rate=1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXAaQcJ6K33m"
      },
      "outputs": [],
      "source": [
        "#Class To Extract Frames From Video (Converting Video File To 3D Array For Processing)\n",
        "class Videoto3D:\n",
        "\n",
        "  def __init__(self, width, height, depth):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.depth = depth\n",
        "\n",
        "  #Skip Frames In The Video For Efficiency\n",
        "  def video3d(self, filename, color=False, skip=True):\n",
        "    cap = cv2.VideoCapture(filename)\n",
        "    nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    if skip:\n",
        "      frames = [x * nframe / self.depth for x in range(self.depth)]\n",
        "    else:\n",
        "      frames = [x for x in range(self.depth)]\n",
        "    framearray = []\n",
        "\n",
        "    for i in range(self.depth):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])\n",
        "      ret, frame = cap.read()\n",
        "      frame = cv2.resize(frame, (self.height, self.width))\n",
        "      if color:\n",
        "        framearray.append(frame)\n",
        "      else:\n",
        "        framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(framearray) #Outputs A Numpy Array For Each Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVw6ET62YUrr"
      },
      "outputs": [],
      "source": [
        "#Function To Load Videos And Labels\n",
        "def loaddata(video_dir, vid3d, nclass, result_dir, color=False, skip=True):\n",
        "\n",
        "  categories = sorted(os.listdir(video_dir)) #One Folder Per Category\n",
        "  X = []\n",
        "  labels = []\n",
        "  labellist = []\n",
        "\n",
        "  total_categories = len(categories)  # For Progress Bar\n",
        "  pbar = tqdm(total=total_categories, desc=\"Processing Directories\")\n",
        "\n",
        "  for category in categories:\n",
        "\n",
        "    if category in ['.DS_Store', 'output'] :\n",
        "      continue\n",
        "\n",
        "    category_path = os.path.join(video_dir, category)\n",
        "\n",
        "    if category not in labellist:\n",
        "      if len(labellist) >= nclass: #Only Process Given Amount Of Classes\n",
        "        break\n",
        "      labellist.append(category)\n",
        "\n",
        "    files = os.listdir(category_path)\n",
        "    for filename in files:\n",
        "\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "\n",
        "      file_path = os.path.join(category_path, filename)\n",
        "      X.append(vid3d.video3d(file_path, color=color, skip=skip))\n",
        "      labels.append(category)\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "  pbar.close()\n",
        "\n",
        "  for num, label in enumerate(labellist): #Assign Numbers For Classes\n",
        "    for i in range(len(labels)):\n",
        "      if label == labels[i]:\n",
        "        labels[i] = num\n",
        "\n",
        "  if color:\n",
        "    return np.array(X).transpose((0, 2, 3, 4, 1)), labels #(num_samples, height, width, depth, channels)\n",
        "  else:\n",
        "    return np.array(X).transpose((0, 2, 3, 1)), labels #(num_samples, height, width, depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97zGUDunaP6m"
      },
      "outputs": [],
      "source": [
        "#Creating A 3D Convolutional Neural Network With Leaky Relu & Batch Normalization\n",
        "def create_3dcnn(input_shape, n_classes):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Input(shape=input_shape))\n",
        "  model.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
        "  model.add(BatchNormalization())  # Standardize activations\n",
        "  model.add(LeakyReLU(negative_slope=0.01)) # Leaky ReLU instead of ReLU\n",
        "\n",
        "  model.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(negative_slope=0.01))\n",
        "\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(negative_slope=0.01))\n",
        "\n",
        "  model.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(negative_slope=0.01))\n",
        "\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(negative_slope=0.01))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RKb0ReFEuNX"
      },
      "outputs": [],
      "source": [
        "#Variables\n",
        "\n",
        "in_dir = '/content/drive/MyDrive/UCF-101'\n",
        "\n",
        "out_dir = '/content/drive/MyDrive/UCF-101/output'\n",
        "if not os.path.isdir(out_dir): #Create Directory If It Doesn't Exist\n",
        "  os.makedirs(out_dir)\n",
        "\n",
        "model_weights = '/content/drive/MyDrive/UCF-101/output/model_weights'\n",
        "if not os.path.isdir(model_weights):\n",
        "  os.makedirs(model_weights)\n",
        "\n",
        "results = '/content/drive/MyDrive/UCF-101/output/results'\n",
        "if not os.path.isdir(results):\n",
        "  os.makedirs(results)\n",
        "\n",
        "n_classes = 101\n",
        "img_rows,img_cols,frames = 32,32,10\n",
        "\n",
        "color = False\n",
        "skip = True\n",
        "\n",
        "channel = 3 if color else 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQotsGPoTU43"
      },
      "outputs": [],
      "source": [
        "#Saving Each Model's Training/Validation Accuracy & Loss\n",
        "def plot_history(history, name):\n",
        "  plt.plot(history.history['acc'], marker='.')\n",
        "  plt.plot(history.history['val_acc'], marker='.')\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.grid()\n",
        "  plt.savefig(os.path.join(results, '{}_accuracy.png'.format(name)))\n",
        "  plt.close()\n",
        "\n",
        "  plt.plot(history.history['loss'], marker='.')\n",
        "  plt.plot(history.history['val_loss'], marker='.')\n",
        "  plt.title('model loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.grid()\n",
        "  plt.savefig(os.path.join(results, '{}_loss.png'.format(name)))\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlqQFFZ4EINy",
        "outputId": "c229a214-870f-455b-9c0d-8a7f0d14341b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_shape:(13320, 32, 32, 10, 1) Y_shape:(13320, 101)\n"
          ]
        }
      ],
      "source": [
        "#Function To Load Training & Testing Data\n",
        "def Train_Test_Data():\n",
        "\n",
        "  vid3d = Videoto3D(img_rows, img_cols, frames)\n",
        "\n",
        "  preloaded_data_path = '/content/drive/MyDrive/UCF-101/output/dataset.npz'\n",
        "\n",
        "  if os.path.exists(preloaded_data_path): #Load Data If It Already Exists\n",
        "    loadeddata = np.load(preloaded_data_path)\n",
        "    X, Y = loadeddata[\"X\"], loadeddata[\"Y\"]\n",
        "  else:\n",
        "    x, y = loaddata(in_dir, vid3d, n_classes, out_dir, color, skip) #Load And Save Data If It Doesn't Exist\n",
        "    X = x.reshape((x.shape[0], img_rows, img_cols, frames, channel))\n",
        "    Y = to_categorical(y, n_classes)\n",
        "\n",
        "    X = X.astype('float16')\n",
        "    np.savez(preloaded_data_path, X=X, Y=Y)\n",
        "  print('X_shape:{} Y_shape:{}'.format(X.shape, Y.shape))\n",
        "\n",
        "  return train_test_split(X, Y, test_size=0.2), X.shape[1: ] #No Random State To Improve Ensemble Performance\n",
        "\n",
        "(X_train, X_test, Y_train, Y_test), in_shape = Train_Test_Data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8it1dErja77O"
      },
      "outputs": [],
      "source": [
        "#Function To Train An Ensemble Of Models\n",
        "def train_model(n_models=10, epochs=100, batch_size=128):\n",
        "\n",
        "  #Train Test Generator\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  #Implementing Model CallBacks\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
        "\n",
        "  #Training Models Individually\n",
        "  models=[]\n",
        "  for i in range(n_models):\n",
        "    print(f\"Model {i}:\")\n",
        "    models.append(create_3dcnn(in_shape, n_classes))\n",
        "    models[-1].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model_path = os.path.join(model_weights, f\"model_{i}.weights.h5\")\n",
        "\n",
        "    #Load Weights If Model Already Trained\n",
        "    if os.path.exists(model_path):\n",
        "      models[-1].load_weights(model_path)\n",
        "      print(f\"Model {i} weights loaded from: {model_path}\")\n",
        "\n",
        "    #Train The Model If Weights Do Not Exist\n",
        "    else:\n",
        "      history = models[-1].fit(train_dataset, validation_data=test_dataset, epochs=epochs, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "      models[-1].save_weights(model_path)\n",
        "      print(f\"Model {i} weights saved at: {model_path}\")\n",
        "      plot_history(history, i)\n",
        "\n",
        "  # models=[]\n",
        "  # for i in range(n_models):\n",
        "  #   print(f\"Model {i}:\")\n",
        "  #   models.append(create_3dcnn(in_shape, n_classes))\n",
        "  #   models[-1].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  #   history = models[-1].fit(train_dataset, validation_data=test_dataset, epochs=epochs, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "  #   model_path = os.path.join(model_weights, f\"model_{i}.weights.h5\")\n",
        "  #   models[-1].save_weights(model_path)\n",
        "  #   print(f\"Model {i} weights saved at: {model_path}\")\n",
        "  #   plot_history(history, i)\n",
        "\n",
        "  #Creating An Ensemble Model\n",
        "  model_inputs = [Input(shape=in_shape) for _ in range (n_models)]\n",
        "  model_outputs = [models[i](model_inputs[i]) for i in range (n_models)]\n",
        "  model_outputs = average(inputs=model_outputs)\n",
        "  model = Model(inputs=model_inputs, outputs=model_outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.save(os.path.join(out_dir, 'ucf101_3dcnnmodel.h5'))\n",
        "\n",
        "  X_test_copies = [np.copy(X_test) for _ in range(n_models)]\n",
        "  loss, acc = model.evaluate(X_test_copies, Y_test, verbose=0)\n",
        "  with open(os.path.join(out_dir, 'result.txt'), 'w') as f:\n",
        "    f.write(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n",
        "\n",
        "  print('merged model:')\n",
        "  print('Test loss:', loss)\n",
        "  print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m-EJoPPnToS"
      },
      "outputs": [],
      "source": [
        "train_model() #Default Hyperparameters"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4u8nURyM6bNGNwdqSRZT3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
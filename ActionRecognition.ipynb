{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2xazwC6R77X16VC1JvWbY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTGtUIc1neV3",
        "outputId": "73872187-cc03-49e2-bddf-9b23d7b97aff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mdu-VOaUKma4"
      },
      "outputs": [],
      "source": [
        "#Required Imports\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "from keras.layers import (Input, Activation, Conv3D, Dense, Dropout, Flatten, MaxPooling3D, Input, average, BatchNormalization, LeakyReLU)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU Availability\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "#Reduce Precision From float32 To float16\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "#Accelerated Linear Algebra\n",
        "tf.config.optimizer.set_jit(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptgr_XSHXmi",
        "outputId": "b89ed43d-c09a-4ba8-ba90-25eec3c8c9a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Class To Extract Frames From Video (Converting Video File To 3D Array For Processing)\n",
        "class Videoto3D:\n",
        "\n",
        "    def __init__(self, width, height, depth):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.depth = depth\n",
        "\n",
        "    #Skip Frames In The Video For Efficiency\n",
        "    def video3d(self, filename, color=False, skip=True):\n",
        "        cap = cv2.VideoCapture(filename)\n",
        "        nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        if skip:\n",
        "            frames = [x * nframe / self.depth for x in range(self.depth)]\n",
        "        else:\n",
        "            frames = [x for x in range(self.depth)]\n",
        "        framearray = []\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])\n",
        "            ret, frame = cap.read()\n",
        "            frame = cv2.resize(frame, (self.height, self.width))\n",
        "            if color:\n",
        "                framearray.append(frame)\n",
        "            else:\n",
        "                framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
        "\n",
        "        cap.release()\n",
        "        return np.array(framearray) #Outputs A Numpy Array For Each Video"
      ],
      "metadata": {
        "id": "RXAaQcJ6K33m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Model Training/Validation Accuracy & Loss\n",
        "\n",
        "#NEEDS FIXING!!!!!!!\n",
        "def plot_history(history, name):\n",
        "    plt.plot(history.history['acc'], marker='.')\n",
        "    plt.plot(history.history['val_acc'], marker='.')\n",
        "    plt.title('model accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['loss'], marker='.')\n",
        "    plt.plot(history.history['val_loss'], marker='.')\n",
        "    plt.title('model loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.grid()\n",
        "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GJna9OKkXg5f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function To Load Videos And Labels\n",
        "def loaddata(video_dir, vid3d, nclass, result_dir, color=False, skip=True):\n",
        "\n",
        "    categories = sorted(os.listdir(video_dir)) #One Folder Per Category\n",
        "    X = []\n",
        "    labels = []\n",
        "    labellist = []\n",
        "\n",
        "    total_categories = len(categories)  # For Progress Bar\n",
        "    pbar = tqdm(total=total_categories, desc=\"Processing Categories\")\n",
        "\n",
        "    for category in categories:\n",
        "\n",
        "      if category == '.DS_Store':\n",
        "                continue\n",
        "\n",
        "      category_path = os.path.join(video_dir, category)\n",
        "\n",
        "      if category not in labellist:\n",
        "            if len(labellist) >= nclass: #Only Process Given Amount Of Classes\n",
        "                break\n",
        "            labellist.append(category)\n",
        "\n",
        "      files = os.listdir(category_path)\n",
        "      for filename in files:\n",
        "\n",
        "        if filename == '.DS_Store':\n",
        "                continue\n",
        "\n",
        "        file_path = os.path.join(category_path, filename)\n",
        "        X.append(vid3d.video3d(file_path, color=color, skip=skip))\n",
        "        labels.append(category)\n",
        "\n",
        "      pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    for num, label in enumerate(labellist): #Assign Numbers For Classes\n",
        "        for i in range(len(labels)):\n",
        "            if label == labels[i]:\n",
        "                labels[i] = num\n",
        "    if color:\n",
        "        return np.array(X).transpose((0, 2, 3, 4, 1)), labels #(num_samples, height, width, depth, channels)\n",
        "    else:\n",
        "        return np.array(X).transpose((0, 2, 3, 1)), labels #(num_samples, height, width, depth)"
      ],
      "metadata": {
        "id": "TVw6ET62YUrr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating A 3D Convolutional Neural Network With Leaky Relu & Batch Normalization\n",
        "\n",
        "def create_3dcnn(input_shape, n_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv3D(32, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n",
        "    model.add(BatchNormalization())  # Normalize activations\n",
        "    model.add(LeakyReLU(alpha=0.01)) # Leaky ReLU instead of ReLU\n",
        "\n",
        "    model.add(Conv3D(32, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "97zGUDunaP6m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables\n",
        "\n",
        "in_dir = '/content/drive/MyDrive/UCF-101'\n",
        "out_dir = '/content/drive/MyDrive/UCF-101/output'\n",
        "\n",
        "if not os.path.isdir(out_dir): #Create Output Directory If It Doesn't Exist\n",
        "      os.makedirs(out_dir)\n",
        "\n",
        "n_classes = 101\n",
        "img_rows,img_cols,frames = 32,32,10\n",
        "\n",
        "color = False\n",
        "skip = True\n",
        "\n",
        "channel = 3 if color else 1"
      ],
      "metadata": {
        "id": "4RKb0ReFEuNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function To Load Training & Testing Data\n",
        "def Train_Test_Data():\n",
        "\n",
        "  vid3d = Videoto3D(img_rows, img_cols, frames)\n",
        "\n",
        "  preloaded_data_path = '/content/drive/MyDrive/UCF-101/output/dataset.npz'\n",
        "\n",
        "  if os.path.exists(preloaded_data_path): #Load Data If It Already Exists\n",
        "      loadeddata = np.load(preloaded_data_path)\n",
        "      X, Y = loadeddata[\"X\"], loadeddata[\"Y\"]\n",
        "  else:\n",
        "      x, y = loaddata(in_dir, vid3d, n_classes, out_dir, color, skip) #Load And Save Data If It Doesn't Exist\n",
        "      X = x.reshape((x.shape[0], img_rows, img_cols, frames, channel))\n",
        "      Y = to_categorical(y, n_classes)\n",
        "\n",
        "      X = X.astype('float32')\n",
        "      np.savez(preloaded_data_path, X=X, Y=Y)\n",
        "  print('X_shape:{} Y_shape:{}'.format(X.shape, Y.shape))\n",
        "\n",
        "  return train_test_split(X, Y, test_size=0.2, random_state=42), X.shape[1: ]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, in_shape = Train_Test_Data()"
      ],
      "metadata": {
        "id": "mlqQFFZ4EINy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function To Train An Ensemble Of Models\n",
        "def train_model(n_models=3, epochs=100, batch_size=128):\n",
        "\n",
        "  #Train Test Generator\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  #Implementing Model CallBacks\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
        "\n",
        "  #Training Models Individually\n",
        "  models=[]\n",
        "  for i in range(n_models):\n",
        "      print('model{}:'.format(i))\n",
        "      models.append(create_3dcnn(in_shape, n_classes))\n",
        "      models[-1].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "      history = models[-1].fit(train_dataset, validation_data=test_dataset, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "      plot_history(history, i) #NEEDS FIXING !!!!!!!!!!!\n",
        "\n",
        "  #Creating An Ensemble Model\n",
        "  model_inputs = [Input(shape=in_shape) for _ in range (n_models)]\n",
        "  model_outputs = [models[i](model_inputs[i]) for i in range (n_models)]\n",
        "  model_outputs = average(inputs=model_outputs)\n",
        "  model = Model(inputs=model_inputs, outputs=model_outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.save_weights(os.path.join(out_dir, 'ucf101_3dcnnmodel.hd5'))\n",
        "\n",
        "  loss, acc = model.evaluate([X_test]*n_models, Y_test, verbose=0)\n",
        "  with open(os.path.join(out_dir, 'result.txt'), 'w') as f:\n",
        "      f.write('Test loss: Test accuracy:{}'.format(loss, acc))\n",
        "\n",
        "  print('merged model:')\n",
        "  print('Test loss:', loss)\n",
        "  print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "8it1dErja77O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model() #Default Hyperparameters"
      ],
      "metadata": {
        "id": "7m-EJoPPnToS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}